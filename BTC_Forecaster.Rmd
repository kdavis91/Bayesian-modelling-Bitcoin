---
title: "BTC forecaster"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r cars,include=FALSE}

#Libraries

library("anytime")
library("bsts")
library("car")
library("caret")
library("forecast")
library("keras")
library("MCMCpack")
library("smooth")
library("tensorflow")
library("tseries")
library("TTR")
library("ggplot2")
library("readr")
```


```{r pressure, echo=FALSE}
#Data

bitstamp <- read.csv("bitstampUSD_1-min_data_2012-01-01_to_2019-03-13.csv", header = TRUE) # Source Bitstamp
head(bitstamp)

```
  
```{r}  

#Wrangle

bitstamp$Time.read <- as.POSIXct(bitstamp$Timestamp, origin = "1970-01-01", tz = "GMT")
bitstamp$Year <- format(as.Date(bitstamp$Time.read, format="%d/%m/%Y"),"%Y")
bitstamp$Month <- format(as.Date(bitstamp$Time.read, format="%d/%m/%Y"),"%m")
bitstamp$YearMonth <- format(as.Date(bitstamp$Time.read, format="%d/%m/%Y"),"%Y-%m")
bitstamp$YearMonthDay <- format(as.Date(bitstamp$Time.read, format="%d/%m/%Y"),"%Y-%m-%d")
bitstamp$Year <- as.factor(bitstamp$Year)
bitstamp$Month <- as.factor(bitstamp$Month)
bitstamp$YearMonth <- as.factor(bitstamp$YearMonth) 
```
  
```{r}  
#Summary

summary(bitstamp)
str(bitstamp)
```



```{r}

# Plots

#Log visualisations

boxplot(log(bitstamp$Volume_.BTC.) ~ bitstamp$YearMonth, range = 0, las = 2, ylab = "Log(Volume BTC)", main = "2011-2019 :: Log(Volume BTC)/(Year + Month) & whiskers at ZERO", col = rainbow(9), ylim = c(-18, 8), cex.axis = .72)

boxplot(log(bitstamp$Volume_.Currency.) ~ bitstamp$YearMonth, las = 2, range = 0, ylab = "Log(Vol Currency)", main = "2011-2019 :: Log(Vol Currency)/(Year + Month) & whiskers at ZERO", col = "yellow", , cex.axis = .72)

boxplot(log(bitstamp$Volume_.BTC.) ~ bitstamp$Month, range = 0, ylab = "Log(Volume BTC)", main = "Log(Volume BTC)/Month cumulated across all years  *Bitstamp USD*", names = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"), at = c(1,2,3, 5,6,7, 9,10,11, 13,14,15), col = "orange")

boxplot(log(bitstamp$Volume_.Currency.) ~ bitstamp$Month, range = 0, ylab = "Log(Volume Currency)", main = "Log(Volume Currency)/Month cumulated across all years  *Bitstamp USD*", names = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"), at = c(1,2,3, 5,6,7, 9,10,11, 13,14,15), col = "yellow")

#Volume indicators ggplot

ggplot(bitstamp, aes(x = Month, fill = Volume_.Currency.))+
  geom_bar(col = "white")+
  facet_wrap(~ Year)+
  ggtitle("2011-2019 :: Volume Currency each Year/Month")+
  xlab("Month")+
  ylab("Volume Currency")

ggplot(bitstamp, aes(x = Year, fill = Volume_.Currency.))+
  stat_count(aes(fill = Volume_.Currency.))+
  facet_wrap(~ Month)+
  ggtitle("2011-2019 :: Volume Currency each Month/Year")+
  xlab("Years")+
  ylab("Volume Currency")+
  theme_minimal()


```



```{r}
# Python script scrapes data and exports .csv 


train <- read.csv(file.choose()) #Select python output to 20190528
test <- read.csv(file.choose()) #Train to 20190728
testdata <- test[,2]
```

```{r}

#Process

train$Date <- as.Date(anytime(train$Date))
test$Date <- as.Date(anytime(test$Date))
train$Volume <- gsub(",", "", train$Volume)
train$Market.Cap <- gsub(",", "", train$Market.Cap)
train$Market.Cap <- as.numeric(train$Market.Cap)
train$Volume <- as.numeric(train$Volume)

#Difference between high and low on each day
a <- matrix(c(0), nrow = 0, ncol = 1)
for(i in 1:nrow(train)){
  a <- rbind(a, train[i,3] - train[i,4])
  i <- i + 1
}
train <- cbind(train,a)

#Volume has missing values#
#Data Manipulation#
fifty_avg <- round(mean(train$Volume[train$a < 50], na.rm = TRUE), digits = 2)
hun_avg <- round(mean(train$Volume[train$a > 50 & train$a < 100], na.rm = TRUE), digits = 2)
hf_avg <- round(mean(train$Volume[train$a > 100 & train$a < 150], na.rm = TRUE), digits = 2)
th_avg <- round(mean(train$Volume[train$a > 150 & train$a < 350], na.rm = TRUE), digits = 2)
for(i in 1:nrow(train)){
  if(is.na(train[i,6])){
    if(train$a[i] < 50){
      train$Volume[i] <- fifty_avg
    } else if(train$a[i] < 100){
      train$Volume[i] <- hun_avg
    } else if(train$a[i] < 150){
      train$Volume[i] <- hf_avg
    } else if(train$a[i] < 350){
      train$Volume[i] <- th_avg
    }else
      print("Uncaught Title")
  }
}
train <- train[, - 8] #Removing column 8
```


```{r}

#Plots
ggplot(train, aes(Date, Close..)) + geom_line() + scale_x_date("year") + ylim(0,20000) + ylab("Closing Price")

#Convert data set to time series
Train <- xts(train[, -1], order.by = as.POSIXct(train$Date)) 
tsr <- ts(Train[,4], frequency = 365.25,start = c(2013,4,27))
plot(Train$Close,type='l',lwd = 1.5,col='red', ylim = c(0,10000), main = "Bitcoin Closing Price")
#checking for trends and seasonality
dects <- decompose(tsr) #Obtaining the trends and seasonality
plot(dects)

```


It can be seen that there is a uniform seasonal variation in the closing price of bitcoins over the years and the trend has been almost constant till the end of 2016. Hence the forecasting is done based on the data from Nov 7, 2016 for standard algorithms and the entire data set for Multilinear and Bayesian regression. The following are the forecasting algorithms used in predicting the price of bitcoins and each of it is reported with its prediction plots and accuracy.  

  
  
  
      
  
  
  
          
  
**Holt’s Forecasting model:**

Holt’s forecasting or Double exponential smoothing method uses exponential smoothing based on the given input values of alpha and beta. Based on this data and the trend, we have chosen alpha as 0.2 and beta as 0.1057 for this model. Initially the model is constructed using Holt’s algorithm and this model is used to predict the forecasted price for bitcoins for the next 10 days. The dark blue line in the graph below displays the forecasted values and the light blue band displays the confidence intervals. 
  
  
    

```{r}
holtt <-  holt(Train[1289:1655,'Close..'], type = "additive", damped = F) #holt forecast values
holtf <- forecast(holtt, h = 10)
holtdf <- as.data.frame(holtf)
head(holtdf)
plot(holtf, ylim = c(0,10000)) 


```
  
  
    
  
    
 
**Exponential Triple Smoothing:**

Exponential triple smoothing is an algorithm which estimates the initial states and smoothing parameters of a forecast by optimizing the likelihood function to find the local minimum and is restricted within a parametric space to make sure data is forecastable. The dark line in the forecasting graph points towards the forecasted values and the light blue band refers to the confidence interval.    


```{r}
ETS <- ets((Train[,'Close..'])) # ETS forecast values
ETSf <- forecast(ETS, h = 10)
etsdf <- as.data.frame(ETSf)
plot(forecast(ETS, h = 10), ylim = c(0,20000)) 
etsp <- predict(ETS, n.ahead = 10, prediction.interval = T, level = 0.95)
```
  
  
  
  
  
        
    
**ARIMA forecasting:**

Autoregressive Integrated Moving Average forecasting is a method extensively used for time series forecasting. It divides the time series data into models based on the number of time lags in the autoregressive model, the differencing and the order of the moving average model. R provides functionality for two kinds of ARIMA models. One is the auto.arima function which automatically fits the data to the best possible order and other one is ARIMA forecasting in which the order needs to be determined based on the ACF and PACF plots. Both the models are created for forecasting and are compared for accuracy.   
   
```{r}    
tsdf <- diff(Train[,4], lag = 2)
tsdf <- tsdf[!is.na(tsdf)]
adf.test(tsdf)
plot(tsdf, type = 1, ylim = c(-1000, 1000))  
```  
   

```{r}   
#ACF AND PACF plots
acf(tsdf)
pacf(tsdf)   
```   
      
   
```{r}
gege <- arima(Train[,4], order = c(4,2,11))
gegef <- as.data.frame(forecast(gege, h = 61))

gegefct <- cbind(test, gegef[,1])
plot(forecast(gege, h = 10), ylim = c(0,10000))
ggplot() + geom_line(data = gegefct, aes(Date, gegefct[,2]), color = "blue") + geom_line(data = gegefct, aes(Date, gegefct[,3]), color = "Dark Red")
```
           
      
   
   
**Forecasting using Bayesian Regression:**

Bayesian linear regression is one of the ways to deal with linear regression. Here, the statistical investigation is attempted inside the setting of Bayesian derivation. The earlier conviction about the parameters is joined with the information's probability work as per Bayes hypothesis to yield the back conviction about the parameters and. This data model is that an information vector x of length m duplicates a coefficient matrix A to create a yield vector y of length d, with Gaussian noise included. This is a conditional model for y just: the appropriation of x isn't required. As we should see, conditional models make a small reference in Bayesian inference. We have created 3 difference Bayesian models-1) Only using the times series of Closing price of bitcoins 2) Simple Bayesian regression using only a single model. 3) 10 different Bayesian models normalized to one.
  
```{r}  
ss <- AddLocalLinearTrend(list(), Train[,4]) #Adding linear trend to model
ss <- AddSeasonal(ss, Train[,4], nseasons = 365) #Adding seasonal trend to model
model1 <- bsts(Train[,4],
               state.specification = ss,
               niter = 10)

plot(model1, ylim = c(0,10000)) #Plot based on bayesian regression of the model
pred1 <- predict(model1, horizon = 10)
plot(pred1, plot.original = 50,ylim = c(0,9000))
pred1$mean
#accuracy(pred1$mean, testdata)

model2 <- bsts(Close.. ~ ., state.specification = ss,
               niter = 10,
               data = as.data.frame(Train))
model3 <- bsts(Close.. ~ ., state.specification = ss,
               niter = 10,
               data = as.data.frame(Train),
               expected.model.size = 10)
CompareBstsModels(list("Model 1" = model1, "Model 2" = model2, "Model 3" = model3), colors = c("blue", "red", "green"))  
```   
   
   
   
   
   
   
     